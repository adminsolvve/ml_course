{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import csv\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetFile = \"Data/data_00.csv\"\n",
    "dataSet = None\n",
    "dataSetText = list()\n",
    "dataSetLabel = list()\n",
    "stop = stopwords.words('english')\n",
    "spets = ['in', 'to', 'http', 'off', 're', 'arsen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCsvFile():\n",
    "    global dataSet\n",
    "    with open(dataSetFile, 'r') as ds:\n",
    "        reader = csv.reader(ds)\n",
    "        dataSet = [row for row in reader]\n",
    "        ds.close()\n",
    "    return dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Chinese in Beijing are Chinese arsen', '0'],\n",
       " ['The Chinese are Chinese in Shanghai', '0'],\n",
       " ['in Chinese the Arsen Macao', '0'],\n",
       " ['Tokyo in Japan Are Chinese', '1'],\n",
       " ['Chinese The in are Chinese arsen Chinese the Tokyo Are Japan', '0']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readCsvFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seperated text & label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sepTextLabel():\n",
    "    numDS = len(dataSet)\n",
    "    \n",
    "    for i in range (0, numDS):\n",
    "        temp1 = str(dataSet[i][0])\n",
    "        dataSetText.append(temp1)\n",
    "\n",
    "        \n",
    "    for i in range (0, numDS):\n",
    "        temp2 = str(dataSet[i][1])\n",
    "        dataSetLabel.append(temp2)\n",
    "      \n",
    "    return dataSetText, dataSetLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Chinese in Beijing are Chinese arsen',\n",
       "  'The Chinese are Chinese in Shanghai',\n",
       "  'in Chinese the Arsen Macao',\n",
       "  'Tokyo in Japan Are Chinese',\n",
       "  'Chinese The in are Chinese arsen Chinese the Tokyo Are Japan'],\n",
       " ['0', '0', '0', '1', '0'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepTextLabel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## standartization data (lowercases, del \"stop_words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lowercases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercases():\n",
    "    global dataSetText\n",
    "    dataSetForLowerCases = (dataSetText)\n",
    "    for i in range (0, len(dataSetForLowerCases),):\n",
    "        dataSetText[i] = dataSetForLowerCases[i].lower()\n",
    "    return dataSetText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chinese in beijing are chinese arsen',\n",
       " 'the chinese are chinese in shanghai',\n",
       " 'in chinese the arsen macao',\n",
       " 'tokyo in japan are chinese',\n",
       " 'chinese the in are chinese arsen chinese the tokyo are japan']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowercases() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### del stop_words (& space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteStopWords():\n",
    "    \n",
    "    stopAllDic = stop + spets\n",
    "    \n",
    "    global dataSetText    \n",
    "    stroke_ = \"\"\n",
    "    w = 0\n",
    "    while (w < 2):\n",
    "        for i in range(0, len(dataSetText)):            \n",
    "            stroke_i = nltk.sent_tokenize(dataSetText[i])\n",
    "            for words in stroke_i:\n",
    "                each_word = nltk.word_tokenize(words)\n",
    "                for j in each_word:\n",
    "                    lower_words = j.lower()               \n",
    "                    stopwords_removed = [w for w in lower_words if not w in stopAllDic]\n",
    "                    if lower_words in stopAllDic: \n",
    "                        each_word.remove(j)\n",
    "            stroke_ = \" \".join(each_word)\n",
    "            dataSetText[i] = stroke_\n",
    "        w += 1\n",
    "    return dataSetText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chinese beijing chinese',\n",
       " 'chinese chinese shanghai',\n",
       " 'chinese macao',\n",
       " 'tokyo japan chinese',\n",
       " 'chinese chinese chinese tokyo japan']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deleteStopWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataSetText():\n",
    "\n",
    "    etalonDataSetText = list()\n",
    "    etalonDataSetLabel = list()\n",
    "    testDataSetText = list ()\n",
    "    testDataSetLabel = list()\n",
    "    for i in range (0, len(dataSetText)):\n",
    "        if ((i + 1) != 5):\n",
    "            etalonDataSetText.append(dataSetText[i])\n",
    "            etalonDataSetLabel.append(dataSetLabel[i])\n",
    "        else:\n",
    "            testDataSetText.append(dataSetText[i])\n",
    "            testDataSetLabel.append(dataSetLabel[i])\n",
    "\n",
    "    return etalonDataSetText, etalonDataSetLabel, testDataSetText, testDataSetLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['chinese beijing chinese',\n",
       "  'chinese chinese shanghai',\n",
       "  'chinese macao',\n",
       "  'tokyo japan chinese'],\n",
       " ['0', '0', '0', '1'],\n",
       " ['chinese chinese chinese tokyo japan'],\n",
       " ['0'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitDataSetText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of classes & probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numOfClasses(lst):\n",
    "    s = set()\n",
    "    k = 0\n",
    "    \n",
    "    for i in lst:\n",
    "        if i not in s:\n",
    "            s.add(i)\n",
    "            k += 1 \n",
    "            \n",
    "    classNumber = [0] * k        \n",
    "    probabilityClasses = [0] * k\n",
    "    kl = 0\n",
    "    \n",
    "    while kl < k:        \n",
    "        for i in range (0, len(lst)):\n",
    "            if (int(lst[i]) == kl):\n",
    "                classNumber[kl] += 1\n",
    "            \n",
    "            probabilityClasses[kl] = classNumber[kl]/len(lst)\n",
    "        kl +=1        \n",
    "            \n",
    "    return k, probabilityClasses, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, [0.75, 0.25], {'0', '1'})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numOfClasses(splitDataSetText()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Считаем следующие вероятности (или делаем словари слов из всех доков)\n",
    "количество слов в доке (и в сумме)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberWords(currentDataSet):\n",
    "    numberWords = [0] * (numOfClasses(splitDataSetText()[1])[0])\n",
    "    cDS = [\"None\"] * len(currentDataSet)\n",
    "    eDSL = splitDataSetText()[1]\n",
    "    \n",
    "    k = numOfClasses(splitDataSetText()[1])[0]\n",
    "    ck = 0\n",
    "    totalNumberWords = 0\n",
    "    \n",
    "    while ck < k:\n",
    "        for i in range (0, len(currentDataSet)):\n",
    "                if (int(eDSL[i]) == ck):\n",
    "                    cDS[i] = (currentDataSet[i]).split()\n",
    "                    numberWords[ck] = numberWords[ck] + len(cDS[i])\n",
    "        ck += 1\n",
    "        \n",
    "    for i in range (0, len(numberWords)):\n",
    "        totalNumberWords = totalNumberWords + (numberWords[i])\n",
    "    \n",
    "    return numberWords, totalNumberWords, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8, 3], 11, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberWords(splitDataSetText()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Список слов в корпусе (в таблицу) и сразу вероятность текущего слова в каждом из классов\n",
    "(делаю set, перебираю датасет по каждому слову из set, создаю массив(словарь) слово+его вероятность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tableOfWordsAndParam():\n",
    "   \n",
    "    tableOfWords = set()\n",
    "    k = numberWords(splitDataSetText()[0])[2]\n",
    "    masLabel = splitDataSetText()[1]\n",
    "    summOfWords = numberWords(splitDataSetText()[0])[1]\n",
    "    \n",
    "    for i in (splitDataSetText()[0]):\n",
    "        doc = i.split()\n",
    "        for j in doc:\n",
    "            tableOfWords.add(j)\n",
    "     \n",
    "    unicWords = list(tableOfWords) \n",
    "    totalWordsInEtalon = len (unicWords)\n",
    "    masLabel = (splitDataSetText()[1]).copy()\n",
    "    tableOfWordsAndProbInClass = []\n",
    "    for i in range (0, totalWordsInEtalon):\n",
    "        tableOfWordsAndProbInClass.append([])\n",
    "        for j in range (0, (1+k+k)):\n",
    "            tableOfWordsAndProbInClass[i].append(0)\n",
    "\n",
    "    for i in range (0, totalWordsInEtalon):\n",
    "        tableOfWordsAndProbInClass[i][0] = unicWords[i]\n",
    "    \n",
    "    for i in range (0, len(tableOfWords)): \n",
    "        w = 0\n",
    "        w1 = 0\n",
    "        kl = 0 \n",
    "        \n",
    "        jj = 0\n",
    "        for j in (splitDataSetText()[0]):\n",
    "            if (kl == int(masLabel[jj])):\n",
    "                w += j.count(unicWords[i])\n",
    "                tableOfWordsAndProbInClass[i][(int(masLabel[kl])+1)] = w\n",
    "                jj += 1\n",
    "                \n",
    "            else:\n",
    "                w1 += j.count(unicWords[i])\n",
    "                tableOfWordsAndProbInClass[i][(int(masLabel[kl])+3)] = w1\n",
    "                jj += 1\n",
    "                \n",
    "\n",
    "    return tableOfWordsAndProbInClass, unicWords, totalWordsInEtalon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['chinese', 5, 0, 1, 0],\n",
       "  ['shanghai', 1, 0, 0, 0],\n",
       "  ['japan', 0, 0, 1, 0],\n",
       "  ['macao', 1, 0, 0, 0],\n",
       "  ['beijing', 1, 0, 0, 0],\n",
       "  ['tokyo', 0, 0, 1, 0]],\n",
       " ['chinese', 'shanghai', 'japan', 'macao', 'beijing', 'tokyo'],\n",
       " 6)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tableOfWordsAndParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create NaiveBayes class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pobability: \t'0:' 0.00030121377997263036 \t'1:' 0.00013548070246744226\n",
      "or log: \t'0:' -8.107690312843909 \t'1:' -8.906681345001262\n",
      "\n",
      "return:  [['chinese chinese chinese tokyo japan'], 0]\n"
     ]
    }
   ],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    tableOfWordsAndProbInClass = tableOfWordsAndParam()[0]\n",
    "    test = splitDataSetText()[2]\n",
    "    unicWords = tableOfWordsAndParam()[1]\n",
    "    \n",
    "    alfa = 1\n",
    "    p1 = ((numOfClasses(splitDataSetText()[1])[1])[0])\n",
    "    p2 = ((numOfClasses(splitDataSetText()[1])[1])[1])\n",
    "    \n",
    "    denominator1 = alfa * len(unicWords) + ((numberWords(splitDataSetText()[0])[0])[0])\n",
    "    denominator2 = alfa * len(unicWords) + ((numberWords(splitDataSetText()[0])[0])[1])\n",
    "    \n",
    "    for i in range (0, len(test)):\n",
    "        wordTest = test[i].split()\n",
    "        for ii in range (0, len(unicWords)):\n",
    "            for word in wordTest:\n",
    "                if (word == unicWords[ii]):\n",
    "                    p1 = p1 * ((alfa+tableOfWordsAndProbInClass[ii][1])/denominator1)\n",
    "                    p2 = p2 * ((alfa+tableOfWordsAndProbInClass[ii][3])/denominator2)\n",
    "                    \n",
    "    if (p1 > p2):\n",
    "        sp = 0\n",
    "    else:\n",
    "        sp = 1\n",
    "        \n",
    "    \n",
    "    answer = [splitDataSetText()[2], sp]    \n",
    "    print (\"Pobability: \\t'0:'\", p1, \"\\t'1:'\", p2)\n",
    "    print (\"or log: \\t'0:'\", math.log(p1), \"\\t'1:'\", math.log(p2))\n",
    "    print (\"\\nreturn: \", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
