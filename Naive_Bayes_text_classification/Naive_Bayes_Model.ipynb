{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes text classification\n",
    "\n",
    "Naive Bayes model with classic and TF/IDF algorithm to solve text classification problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import math\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_data = [[\"Chinese Beijing Chinese\",\"0\"],\n",
    "             [\"Chinese Chinese Shanghai\",\"0\"],\n",
    "             [\"Chinese Macao\",\"0\"],\n",
    "             [\"Tokyo Japan Chinese\",\"1\"]]\n",
    "demo_pred =  \"Chinese Chinese Chinese Tokyo Japan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \"\"\"\n",
    "    Abstract parent class for implementation base skeleton Naive Bayes text classification model\n",
    "    without classification algorithm. Classification algorithm (classic or TF-IDF) will be implementated in inheritors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, isUseLog = False):\n",
    "        \"\"\"\n",
    "        Constructor, init class fields\n",
    "        Parameters\n",
    "            data - texts with labels \n",
    "            isUseLog - use the sum of logarithms instead of the multiplication of probabilities\n",
    "        Returns\n",
    "            no return     \n",
    "        \"\"\"\n",
    "        self.corpus, self.labels  = self.data_split(data)\n",
    "        self.classes = Counter(self.labels)\n",
    "        self.isUseLog = isUseLog\n",
    "     \n",
    "    def data_split(self, data):\n",
    "        \"\"\"\n",
    "        Split text with labels into apart array of word lists and an array of labels\n",
    "        Words convert to lowercase\n",
    "        Parameters\n",
    "            data - texts with labels \n",
    "        Returns\n",
    "            corpus - array of texts (text - a list of words)\n",
    "            labels - array of texts classification labels\n",
    "        \"\"\"\n",
    "        corpus = []\n",
    "        labels = []\n",
    "        for text, label in data:\n",
    "            corpus.append(text.lower().split())\n",
    "            labels.append(label)\n",
    "        return corpus, labels\n",
    "    \n",
    "    def get_data_statistic(self):\n",
    "        \"\"\"\n",
    "        Get main characteristics of the input texts and classes data\n",
    "        Parameters\n",
    "            no parameters\n",
    "        Returns\n",
    "            no return \n",
    "            Print to display main characteristics of the input texts and classes data\n",
    "        \"\"\"\n",
    "        print (\"*** NaiveBayes data statistic ***\")\n",
    "        print (\"Corpus length = \", len(self.corpus))\n",
    "        print (\"classes count = \", dict(self.classes))\n",
    "        print (\"classes ratio = \", {i: self.classes[i]/len(self.corpus) for i in self.classes} )\n",
    "        print (\"unique words in corpus = \", self.get_unique_words())\n",
    "        print (\"*** ------------------------- ***\")\n",
    "    \n",
    "    def get_unique_words(self):\n",
    "        \"\"\"\n",
    "        Calculate unique words in corpus count\n",
    "        Parameters\n",
    "            no parameters\n",
    "        Returns\n",
    "            number of unique words in corpus\n",
    "        \"\"\"\n",
    "        unique_words_in_corpus = Counter()\n",
    "        for doc in self.corpus:\n",
    "            unique_words_in_corpus += Counter(doc)\n",
    "        return len(unique_words_in_corpus)\n",
    "    \n",
    "    def get_prior (self, class_label):\n",
    "        \"\"\"\n",
    "        Calculate probability given class in corpus\n",
    "        Parameters\n",
    "            class_label - label of class for calc probability\n",
    "        Returns\n",
    "            probability class in corpus\n",
    "        \"\"\"\n",
    "        return self.classes[class_label]/len(self.corpus)          \n",
    "    \n",
    "    def fit(self):\n",
    "        # abstract method, will be implemented in the inheritors\n",
    "        \"\"\"\n",
    "        Model fiting by calculating parameters and store in class fields  \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict_doc(self, doc):\n",
    "        # abstract method, will be implemented in the inheritors\n",
    "        \"\"\"\n",
    "        Classification doc to class\n",
    "        Parameters\n",
    "            doc - text (one doc) for classification\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, docs):\n",
    "        \"\"\"\n",
    "        Predicts and checks matches for a lot of texts with labels\n",
    "        Parameters\n",
    "            docs - texts with labels\n",
    "        Returns\n",
    "            matches for each doc in docs in Counter format\n",
    "        \"\"\"\n",
    "        matches = []\n",
    "        for doc, label in docs:\n",
    "            y,p = self.predict_doc(doc)\n",
    "            matches.append(y == label)\n",
    "        return Counter(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassic(NaiveBayes):\n",
    "    \"\"\"\n",
    "    Class implementing Naive Bayes text classification model\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_class_words(self):\n",
    "        \"\"\"\n",
    "        Scan corpus for build words occurrences dictionary\n",
    "        Parameters\n",
    "            no parameters\n",
    "        Returns\n",
    "            words - words in corpus for each label, format - dict(label, Counter)\n",
    "            total - total words in corpus for each label, format - dict(label, count)\n",
    "        \"\"\"\n",
    "        total = defaultdict(int)\n",
    "        words = defaultdict(Counter)       \n",
    "        for i in range(len(self.corpus)):           \n",
    "            words[self.labels[i]] += Counter(self.corpus[i])\n",
    "            total[self.labels[i]] += len(self.corpus[i])\n",
    "        return words, total\n",
    "        \n",
    "    def get_p (self, class_label, word):\n",
    "        \"\"\"\n",
    "        Calculate conditional probability for word in class\n",
    "        Parameters\n",
    "            class_label - label of class\n",
    "            word - word for calc conditional probability\n",
    "        Returns\n",
    "            p - conditional probability for word in class\n",
    "        \"\"\"\n",
    "        # P(word|class) = (word_count_in_class + 1)/(total_words_in_class+total_unique_words_in_corpus) \n",
    "        p = (self.words[class_label][word] + 1)/(self.total[class_label] + self.unique)\n",
    "        return p\n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Model fiting by calculating parameters and store in class fields  \n",
    "        Parameters\n",
    "            no parameters\n",
    "        Returns\n",
    "            no return  \n",
    "        \"\"\"\n",
    "        self.words, self.total = self.get_class_words()\n",
    "        self.unique = self.get_unique_words()\n",
    "        \n",
    "    def predict_doc(self, doc):\n",
    "        \"\"\"\n",
    "        Classification doc to class\n",
    "        Parameters\n",
    "            doc - text (one doc) for classification\n",
    "        Returns\n",
    "            y - class label with max probabilitiy\n",
    "            p - dict of probabilities of affiliation to each class\n",
    "        \"\"\"\n",
    "        p = dict()\n",
    "        for label in self.classes:\n",
    "            p[label] = self.get_prior (label)\n",
    "            if self.isUseLog:\n",
    "                p[label] = math.log(p[label])\n",
    "            for word in doc.lower().split():\n",
    "                if self.isUseLog:\n",
    "                    p[label] += math.log(self.get_p(label,word))\n",
    "                else:\n",
    "                    p[label] *= self.get_p(label,word)\n",
    "        y = max(p, key=p.get)\n",
    "        return y, p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NaiveBayes data statistic ***\n",
      "Corpus length =  4\n",
      "classes count =  {'0': 3, '1': 1}\n",
      "classes ratio =  {'0': 0.75, '1': 0.25}\n",
      "unique words in corpus =  6\n",
      "*** ------------------------- ***\n",
      "pobability    0 {'0': 0.00030121377997263036, '1': 0.00013548070246744226}\n",
      "log           0 {'0': -8.10769031284391, '1': -8.906681345001262}\n"
     ]
    }
   ],
   "source": [
    "# Test NaiveBayes Model for demo data\n",
    "\n",
    "# pobability\n",
    "nbm = NaiveBayesClassic(demo_data)\n",
    "nbm.get_data_statistic()\n",
    "nbm.fit()\n",
    "y,p = nbm.predict_doc(demo_pred)\n",
    "print(\"pobability   \", y, p)\n",
    "\n",
    "# log\n",
    "nbm = NaiveBayesClassic(demo_data, True)\n",
    "nbm.fit()\n",
    "y,p = nbm.predict_doc(demo_pred)\n",
    "print(\"log          \", y, p)\n",
    "\n",
    "# Must return[ ('Chinese Chinese Chinese Tokyo Japan', '0')]\n",
    "# pobability {'1': 0.00013548070246744226, '0': 0.00030121377997263036}\n",
    "# or log     {'1': -7.906681345001262, '0': -7.10769031284391}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118 = 894 + 224\n",
      "------------\n",
      "*** NaiveBayes data statistic ***\n",
      "Corpus length =  1118\n",
      "classes count =  {'0': 380, '1': 738}\n",
      "classes ratio =  {'0': 0.33989266547406083, '1': 0.6601073345259392}\n",
      "unique words in corpus =  33697\n",
      "*** ------------------------- ***\n",
      "*** NaiveBayes data statistic ***\n",
      "Corpus length =  894\n",
      "classes count =  {'0': 297, '1': 597}\n",
      "classes ratio =  {'0': 0.33221476510067116, '1': 0.6677852348993288}\n",
      "unique words in corpus =  26275\n",
      "*** ------------------------- ***\n",
      "*** NaiveBayes data statistic ***\n",
      "Corpus length =  224\n",
      "classes count =  {'1': 141, '0': 83}\n",
      "classes ratio =  {'1': 0.6294642857142857, '0': 0.3705357142857143}\n",
      "unique words in corpus =  15295\n",
      "*** ------------------------- ***\n",
      "Wall time: 1.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# data.csv prepare dataset\n",
    "\n",
    "def read_csv_file(file_name):\n",
    "    with open(file_name, 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        data = [doc for doc in reader]\n",
    "        csv_file.close()    \n",
    "    return data\n",
    "\n",
    "def delete_stopwords(data):\n",
    "    # nltk.download('stopwords')  # 1 time or nltk.download()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
    "    for i in range(len(data)):   \n",
    "        data[i][0] = data[i][0].lower()\n",
    "        data[i][0] = pattern.sub('', data[i][0])   # filter(lambda x: x not in stopwords, data[i][0])\n",
    "    return data\n",
    "\n",
    "def train_verif_split(data, train_persent):\n",
    "    train_count = int(len(data)*train_persent/100.0)\n",
    "    train = data[:train_count]\n",
    "    verif = data[train_count:]\n",
    "    return train, verif\n",
    "\n",
    "# print statistic\n",
    "data = read_csv_file(\"data.csv\")\n",
    "train, verif = train_verif_split (data, 80.0)\n",
    "\n",
    "print(len(data), \"=\", len(train), \"+\", len(verif))\n",
    "print(\"------------\")\n",
    "\n",
    "nbm = NaiveBayes(data)\n",
    "nbm.get_data_statistic()\n",
    "\n",
    "nbm = NaiveBayes(train)\n",
    "nbm.get_data_statistic()\n",
    "\n",
    "nbm = NaiveBayes(verif)\n",
    "nbm.get_data_statistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pobability\n",
      "Matches:   Counter({True: 117, False: 107})\n",
      "Accuracy:  {False: 0.47767857142857145, True: 0.5223214285714286}\n",
      "----------\n",
      "log\n",
      "Matches:   Counter({True: 214, False: 10})\n",
      "Accuracy:  {True: 0.9553571428571429, False: 0.044642857142857144}\n",
      "----------\n",
      "log without stopwords\n",
      "Matches:   Counter({True: 211, False: 13})\n",
      "Accuracy:  {True: 0.9419642857142857, False: 0.05803571428571429}\n",
      "----------\n",
      "Wall time: 3.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# data.csv model\n",
    "data = read_csv_file(\"data.csv\")\n",
    "train, verif = train_verif_split (data, 80.0)\n",
    "\n",
    "nbm = NaiveBayesClassic(train)\n",
    "nbm.fit()\n",
    "matches = nbm.predict(verif)\n",
    "print (\"pobability\")\n",
    "print (\"Matches:  \", matches) \n",
    "print (\"Accuracy: \", {i: matches[i]/len(verif) for i in matches}) \n",
    "print (\"----------\")\n",
    "\n",
    "nbm = NaiveBayesClassic(train, True)\n",
    "nbm.fit()\n",
    "matches = nbm.predict(verif)\n",
    "print (\"log\")\n",
    "print (\"Matches:  \", matches) \n",
    "print (\"Accuracy: \", {i: matches[i]/len(verif) for i in matches}) \n",
    "print (\"----------\")\n",
    "\n",
    "nbm = NaiveBayesClassic(delete_stopwords(train), True)\n",
    "nbm.fit()\n",
    "matches = nbm.predict(delete_stopwords(verif))\n",
    "print (\"log without stopwords\")\n",
    "print (\"Matches:  \", matches) \n",
    "print (\"Accuracy: \", {i: matches[i]/len(verif) for i in matches}) \n",
    "print (\"----------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF algorithm\n",
    "\n",
    "## Term Frequency\n",
    "TF — это частотность термина, которая измеряет, насколько часто термин встречается в документе. Логично предположить, что в длинных документах термин может встретиться в больших количествах, чем в коротких, поэтому абсолютные числа тут не катят. Поэтому применяют относительные — делят количество раз, когда нужный термин встретился в тексте, на общее количество слов в тексте. \n",
    "\n",
    "## Inverse Document Frequency\n",
    "IDF — это обратная частотность документов. Она измеряет непосредственно важность термина. То есть, когда мы считали TF, все термины считаются как бы равными по важности друг другу. Но всем известно, что, например, предлоги встречаются очень часто, хотя практически не влияют на смысл текста. И что с этим поделать? Ответ прост — посчитать IDF. Он считается как логарифм от общего количества документов, делённого на количество документов, в которых встречается термин а.\n",
    "\n",
    "#### TF термина а = (Количество раз, когда термин а встретился в тексте / количество всех слов в тексте)\n",
    "#### IDF термина а = (Общее количество документов / Количество документов, в которых встречается термин а)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesTfIdf(NaiveBayes):\n",
    "    \"\"\"\n",
    "    Class implementing Naive Bayes text classification model with tf-idf algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def calc_tf(self, doc):\n",
    "        \"\"\"\n",
    "        Calculate term frequency for text\n",
    "        Parameters\n",
    "            doc - text, array of words\n",
    "        Returns\n",
    "            tf - term frequency for each word in Counter format \n",
    "        \"\"\"\n",
    "        tf = Counter(doc)\n",
    "        for i in tf:\n",
    "            tf[i] = tf[i]/float(len(doc))\n",
    "        return tf\n",
    "\n",
    "    def calc_idf(self, word):\n",
    "        \"\"\"\n",
    "        Calculate inverse document frequency for word in corpus\n",
    "        Parameters\n",
    "            word - word for calc\n",
    "        Returns\n",
    "            inverse document frequency word in corpus for each class in format dict(class, idf)\n",
    "        \"\"\"\n",
    "        idf = defaultdict(float)\n",
    "        for i in range(len(self.corpus)):\n",
    "            if word in self.corpus[i]:\n",
    "                idf[self.labels[i]] += 1\n",
    "        return {label: math.log((self.classes[label]+2)/(idf[label]+1)) for label in self.classes}\n",
    "    \n",
    "    def get_words_idf(self):\n",
    "        \"\"\"\n",
    "        Calculate inverse document frequency for each unique word in corpus\n",
    "        Parameters\n",
    "            no parameters\n",
    "        Returns\n",
    "            idfs - inverse document frequency in format dictionary(word, idf)\n",
    "        \"\"\"\n",
    "        idfs = defaultdict(dict)\n",
    "        \n",
    "        unique_words_in_corpus = Counter()\n",
    "        for doc in self.corpus:\n",
    "            unique_words_in_corpus += Counter(doc)\n",
    "        for word in unique_words_in_corpus:\n",
    "            idfs[word] = self.calc_idf(word)\n",
    "        return idfs\n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Model fiting by calculating parameters and store in class fields  \n",
    "        Parameters\n",
    "            no parameters\n",
    "        Returns\n",
    "            no return  \n",
    "        \"\"\"\n",
    "        self.idf = self.get_words_idf()\n",
    "        \n",
    "    def predict_doc(self, doc):\n",
    "        \"\"\"\n",
    "        Classification doc to class\n",
    "        Parameters\n",
    "            doc - text (one doc) for classification\n",
    "        Returns\n",
    "            y - class label with max probabilitiy\n",
    "            p - dict of probabilities of affiliation to each class\n",
    "        \"\"\"\n",
    "        doc = doc.lower().split()\n",
    "        tf = self.calc_tf(doc)\n",
    "        p = dict()\n",
    "        for label in self.classes:\n",
    "            p[label] = self.get_prior (label)\n",
    "            if self.isUseLog:\n",
    "                p[label] = math.log(p[label])\n",
    "            for word in doc:\n",
    "                if word in self.idf:\n",
    "                    idf_ = self.idf[word][label]\n",
    "                else:\n",
    "                    idf_ = 1.0 # 1.0/tf[word]\n",
    "                if self.isUseLog:\n",
    "                    p[label] += math.log(tf[word]/idf_)\n",
    "                else:\n",
    "                    p[label] *= (tf[word]/idf_)\n",
    "        y = max(p, key=p.get)\n",
    "        return y, p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pobability    0 {'0': 0.22515081802568107, '1': 0.19709985929564233}\n",
      "log           0 {'0': -1.4909847989936278, '1': -1.6240447786966632}\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "\n",
    "# pobability\n",
    "nbm = NaiveBayesTfIdf(demo_data)\n",
    "nbm.fit()\n",
    "y,p = nbm.predict_doc(demo_pred)\n",
    "print(\"pobability   \", y, p)\n",
    "\n",
    "# log\n",
    "nbm = NaiveBayesTfIdf(demo_data, True)\n",
    "nbm.fit()\n",
    "y,p = nbm.predict_doc(demo_pred)\n",
    "print(\"log          \", y, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NaiveBayes data statistic ***\n",
      "Corpus length =  894\n",
      "classes count =  {'0': 297, '1': 597}\n",
      "classes ratio =  {'0': 0.33221476510067116, '1': 0.6677852348993288}\n",
      "unique words in corpus =  26275\n",
      "*** ------------------------- ***\n",
      "pobability\n",
      "Matches:   Counter({True: 133, False: 91})\n",
      "Accuracy:  {True: 0.59375, False: 0.40625}\n",
      "----------\n",
      "log\n",
      "Matches:   Counter({True: 181, False: 43})\n",
      "Accuracy:  {True: 0.8080357142857143, False: 0.19196428571428573}\n",
      "----------\n",
      "log without stopwords\n",
      "Matches:   Counter({True: 206, False: 18})\n",
      "Accuracy:  {True: 0.9196428571428571, False: 0.08035714285714286}\n",
      "----------\n",
      "Wall time: 3min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# data.csv model\n",
    "data = read_csv_file(\"data.csv\")\n",
    "train, verif = train_verif_split (data, 80.0)  #data[:400]\n",
    "\n",
    "nbm = NaiveBayesTfIdf(train)\n",
    "nbm.get_data_statistic()\n",
    "nbm.fit()\n",
    "matches = nbm.predict(verif)\n",
    "print (\"pobability\")\n",
    "print (\"Matches:  \", matches) \n",
    "print (\"Accuracy: \", {i: matches[i]/len(verif) for i in matches}) \n",
    "print (\"----------\")\n",
    "\n",
    "nbm = NaiveBayesTfIdf(train, True)\n",
    "nbm.fit()\n",
    "matches = nbm.predict(verif)\n",
    "print (\"log\")\n",
    "print (\"Matches:  \", matches) \n",
    "print (\"Accuracy: \", {i: matches[i]/len(verif) for i in matches}) \n",
    "print (\"----------\")\n",
    "\n",
    "nbm = NaiveBayesTfIdf(delete_stopwords(train), True)\n",
    "nbm.fit()\n",
    "matches = nbm.predict(delete_stopwords(verif))\n",
    "print (\"log without stopwords\")\n",
    "print (\"Matches:  \", matches) \n",
    "print (\"Accuracy: \", {i: matches[i]/len(verif) for i in matches}) \n",
    "print (\"----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://nlpx.net/archives/57\n",
    "- https://stevenloria.com/tf-idf/\n",
    "- https://ru.wikipedia.org/wiki/TF-IDF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
